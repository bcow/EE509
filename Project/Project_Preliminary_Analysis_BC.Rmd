---
title: "Project Preliminary Analysis"
author: "Betsy Cowdery"
date: "November 21, 2014"
output: html_document
---

```{r, echo=FALSE, warning=FALSE, message=FALSE}
## Set up data
library("rjags")
library(coda)
library(mvtnorm)

#setwd(paste0(getwd(),"/Project/"))
glopnet <- read.csv("glopnet.csv")
glopnet[c("X")] <- NULL
glopnet[c("X.1")] <- NULL
glopnet[c("X.2")] <- NULL
glopnet[c("X.3")] <- NULL

gdata.na <- as.data.frame(cbind(
  glopnet$log.LL,
  glopnet$log.LMA,
  glopnet$log.Amass,
  glopnet$log.Nmass,
  glopnet$log.Pmass,
  glopnet$log.Rdmass 
#   glopnet$Species, 
#   glopnet$BIOME,
#   glopnet$Dataset
  ))
colnames(gdata.na)<-c("Log.LL","Log.LMA","Log.Amass","Log.Nmass","Log.Pmass","Log.Rmass")
gdata <- na.omit(gdata.na)

run = TRUE
if(file.exists("jags.outputs.Rdata")){
  load("jags.outputs.Rdata")
  run = FALSE
  }
```

# Goal 

The  goal of this project is to develop a multivariate Bayesian meta-analytical model that synthesizes plant trait data from multiple studies while accounting for various sources of uncertainty. Using observed sample mean, sample size, and a sample error statistics for multiple plant traits, we aim to produced well constrained estimates of mean and precision for a single trait. This has be done in PEcAn (the Predictive Ecosystem Analyzer) using a univariate model [1], however, a multivariate model can leverage the fact that many plant traits are highly correlated [2] to constrain our estimates even further. This may be especially useful improving predictions for studies in which observations are missing. 

For this project I am using the data set compiled by Wright et. all to develop a "leaf economics spectrum" [2]. This data is from the global plant trait network (Glopnet), a database created to quantify leaf economics across the worldâ€™s plant species. 

From this data set I will be focusing on six plant-traits:

(1) Leaf mass per area (LMA) 

(2) Photosynthetic capacity (Amass) - photosynthetic assimilation rates measured under high light, ample soil moisture and ambient CO2

(3) Leaf nitrogen (N) 

(4) Leaf phosphorus (P)

(5) Dark respiration rate (Rmass)  

(6) Leaf lifespan (LL) 

# Data

#### Including studies with missing observations 
```{r,echo=FALSE, fig.width=10, fig.height=10}
pairs(gdata.na, panel=function(x,y){
  points(x,y)
  fit <- lm(y~x)
  p <- pf(summary(fit)$fstatistic[1],summary(fit)$fstatistic[2],summary(fit)$fstatistic[3], lower.tail = F)
	if(p < .01){abline(fit, col='red',lwd=2)}
  # legend("top", legend=sprintf("R2 = %.2f",summary(fit)$r.squared), text.col="blue")
})
```

Here we have plotted each the log of each plant trait variable against one another with a regression line in red if the regression is statistically significant. We can see that each plant trait has a statistically significant ($p < .01$) correlation with all the others, which suggests that a multivariate analysis will in fact be informative. 


#### Excluding studies with missing observations 
```{r,echo=FALSE, fig.width=10, fig.height=10}
pairs(gdata, panel=function(x,y){
  points(x,y)
  fit <- lm(y~x)
  p <- pf(summary(fit)$fstatistic[1],summary(fit)$fstatistic[2],summary(fit)$fstatistic[3], lower.tail = F)
  if(p < .01){abline(fit, col='red',lwd=2)}
  #legend("top", legend=sprintf("R2 = %.2f",summary(fit)$r.squared), text.col="blue")
})
```

Here we have the exact same plot as above, but only including the studies that do not contain missing observations. Clearly there is less data to work with and thus the R-squared values are lower and one pair of traits no longer have a statistically significant correlation. However, I expect that given the presence of so many statistically significant correlations, the multivariate model will provide improved predictions for the means of each variable. 

# Univariate Model

Let $Y_{i,j}$ represent the observed value of the $j$th trait variable in study $i$.

#### Model Graph
```{r, echo=FALSE, fig.width=5}
library(png)
library(grid)
img <- readPNG("Univariate.png")
 grid.raster(img)
```

#### Model in JAGS
```{r, eval=FALSE}
model{
  prec.sigma~dgamma(.001,.001)
  sigma <- 1/prec.sigma
  for(i in 1:n){mu[i]~dnorm(0,.001)}
  
  for(i in 1:N){
    for(j in 1:n){
      Y[i,j]~dnorm(mu[j],prec.sigma)
      }
    }
  }
```

```{r, echo=FALSE,}
if(run){
  UnivModel = "
  model{
  prec.sigma~dgamma(.001,.001)
  sigma <- 1/prec.sigma
  for(i in 1:n){mu[i]~dnorm(0,.001)}
  
  for(i in 1:N){
  for(j in 1:n){
  Y[i,j]~dnorm(mu[j],prec.sigma)
  }
  }
  }"
  
  
  # Without na's
  j.data <- gdata
  N=dim(j.data)[1]; n=dim(j.data)[2]
  data = list(Y=j.data, N=N, n=n)
  init = NULL
  j.model   <- jags.model (file = textConnection(UnivModel),data = data,inits = init, n.chains = 3)
  update(j.model, n.iter=1000)
  j.out   <- coda.samples (model = j.model,variable.names= c("mu","prec.Sigma"), n.iter = 10000)
  out1 = j.out
  
  
  # With na's
  j.data <- gdata.na
  N=dim(j.data)[1]; n=dim(j.data)[2]
  data = list(Y=j.data, N=N, n=n)
  init = NULL
  j.model   <- jags.model (file = textConnection(UnivModel),data = data,inits = init, n.chains = 3)
  update(j.model, n.iter=1000)
  j.out   <- coda.samples (model = j.model,variable.names= c("mu","prec.Sigma"), n.iter = 10000)
  out3 = j.out
  }

out1.df <- as.data.frame(as.matrix(out1))
out3.df <- as.data.frame(as.matrix(out3))

colnames(out1.df)<-colnames(out3.df)<-colnames(gdata)

outs <- list(out1.df,NA,out3.df,NA)

# for(j in 1:6){
#   ranges <-  apply(cbind(outs[[1]][,j],outs[[3]][,j]), 2,
#                    function(x) { dens <- density(x); c(range(dens$x), range(dens$y)) })
#   plot(density(outs[[1]][,j]),
#        xlim = range(ranges[1:2, ]), ylim = range(ranges[3:4, ]),
#        main=paste(names(gdata)[j]))
#   if(j==1 | j ==2){position="topright"}else{position="topleft"}
#   legend(position,legend = c("Uni w/o NA's ", "Uni w/ NA's " ), lty = c(1,1), col=c(1,6))
#   lines(density(outs[[3]][,j]), col=6)
#   }


```


# Multivariate Model

Let 

$Y_{i}$ represent the vector of observed value of the $j$ traits variable in study $i$.

$Y^0_{i,j}$ represent the observed value $j$th trait variable in study $i$, taking into account observation error.

#### Model Graph
```{r, echo=FALSE, fig.width=6}
library(png)
library(grid)
img <- readPNG("Multivariate.png")
 grid.raster(img)
```

#### Model in JAGS
```{r, eval=FALSE}
model{
  prec.Sigma~dwish(Vsig[,],n)
  Sigma[1:n,1:n] <- inverse(prec.Sigma[,])
  
  mu[1:n]~dmnorm(mu0[],Vmu)
  
  for(i in 1:N){
    Y[i,1:n]~dmnorm(mu[],prec.Sigma[,])
    for(j in 1:n){
      X[i,j]~dnorm(Y[i,j],10000000)
      }
    }
  }
```

```{r, echo=FALSE}
if(run){
MultModel = "
model{
  prec.Sigma~dwish(Vsig[,],n)
  Sigma[1:n,1:n] <- inverse(prec.Sigma[,])

  mu[1:n]~dmnorm(mu0[],Vmu)

  for(i in 1:N){
    Y[i,1:n]~dmnorm(mu[],prec.Sigma[,])
    for(j in 1:n){
      X[i,j]~dnorm(Y[i,j],10000000)
    }
  }
}"

# Without na's 
j.data <- gdata
N=dim(j.data)[1]; n=dim(j.data)[2]
data = list(Y=j.data, N=N, n=n, Vsig = diag(n), mu0 = rep(0,n), Vmu = diag(.001,n))
init = NULL
j.model   <- jags.model (file = textConnection(MultModel),data = data,inits = init,n.chains = 3)
update(j.model, n.iter=1000)
j.out   <- coda.samples (model = j.model,variable.names= c("mu","Sigma"),n.iter = 10000)
out2 = j.out


# With na's 

j.data <- gdata.na
N=dim(j.data)[1]; n=dim(j.data)[2]
data = list(X=j.data, N=N, n=n, Vsig = diag(n), mu0 = rep(0,n), Vmu = diag(.001,n))
init = NULL
j.model   <- jags.model (file = textConnection(MultModel), data = data, inits = init, n.chains = 3)
update(j.model, n.iter=1000)
j.out   <- coda.samples (model = j.model, variable.names= c("mu","Sigma"), n.iter = 10000)
out4 = j.out


}

out2.df <- as.data.frame(as.matrix(out2))
out4.df <- as.data.frame(as.matrix(out4))

colnames(out2.df)<-colnames(out4.df)<-colnames(gdata)

outs <- list(out1.df,out2.df,out3.df,out4.df)

# for(j in 1:6){
#   ranges <-  apply(cbind(outs[[2]][,j],outs[[4]][,j]), 2,
#                    function(x) { dens <- density(x); c(range(dens$x), range(dens$y)) })
#   plot(density(outs[[2]][,j]),
#        xlim = range(ranges[1:2, ]), ylim = range(ranges[3:4, ]),
#        main=paste(names(gdata)[j]))
#   if(j==1 | j ==2){position="topright"}else{position="topleft"}
#   legend(position,legend = c("Uni w/o NA's ", "Uni w/ NA's " ), lty = c(1,1), col=c(1,6))
#   lines(density(outs[[4]][,j]), col=6)
#   }


```

```{r,echo=FALSE}
# if{run}{
#   save(out1,out2,out3,out4, file="jags.outputs.Rdata")
# }
```





# Comparisons

#### Overall comparison of computed and estimated means for each of the 6 variables 
```{r, echo=FALSE, fig.height=10,fig.width=10}

mus <- as.data.frame(matrix(NA, 5,6))
mus[1,] <- colMeans(gdata)
for(i in 1:4){
  for(j in 1:6){
        mus[i+1,j] <- mean(outs[[i]][,j])
  }
}

rownames(mus) <- c("Data ","Univariate   without NA's ", "Multivariate without NA's ", "Univariate   with NA's",  "Multivariate with NA's" )
colnames(mus) <- colnames(gdata) 
options(digits=4)
print(mus)

par(mfrow = c(3,2))
for(j in 1:6){
  ranges <-  apply(cbind(outs[[1]][,j],outs[[2]][,j],outs[[3]][,j],outs[[4]][,j]), 2,
                   function(x) { dens <- density(x); c(range(dens$x), range(dens$y)) })
  plot(density(outs[[1]][,j]), col="red", lty=2,
       xlim = range(ranges[1:2, ]), ylim = range(ranges[3:4, ]),
       main=paste(names(gdata)[j]))
  if(j==1 | j ==2){position="topright"}else{position="topleft"}
  legend(position,legend = c( "Multi w/ NA's","Uni   w/ NA's", "Mult w/o NA's ", "Uni  w/o NA's " ), lty = c(1,1,2,2), col=c("blue","red","blue","red"))
  lines(density(outs[[2]][,j]), col="blue", lty=2)
  lines(density(outs[[3]][,j]), col="red")
  lines(density(outs[[4]][,j]), col="blue")
  }
```

When using data that excludes all studies with missing observations, there is practically no difference between the two model's estimated means for each of the variables. However, for both models, including studies with NA's produces estimated means that are significantly different from those produced with data excluding NA's. For the variables Log.LMA and Log.Amass, the estimated means from the univariate and multivariate models were very close, but for the remaining variables,  they were noticeably different, with the estimated mean from the univariate model always closer to the data mean than the estimated mean from the multivariate model. 

#### Error around the mean

```{r, echo=FALSE}
options(digits=4)
for(j in 1:6){
  cat("","",colnames(gdata)[j],sep="\n")
  summary <- as.data.frame(matrix(NA,5,4))
  summary[1,] <- c(mean(gdata[,j]), sd(gdata[,j])/sqrt(length(gdata[,j])), quantile(gdata[,j], c(.025, .975)))
  for(i in 1:4){ 
    summary[i+1,] <- c(mean(outs[[i]][,j]), sd(outs[[i]][,j])/sqrt(length(outs[[i]][,j])), quantile(outs[[i]][,j], c(.025, .975)))
    }
  
  rownames(summary) <- c("Data ","Univariate   without NA's ", "Multivariate without NA's ", "Univariate   with NA's",  "Multivariate with NA's" )
  colnames(summary) <- c("Mean", "SE", "2.5%", "97.5%")
  print(summary)
  
  cat("lowest SE:", rownames(summary)[which(summary$SE==min(summary$SE))])
  }

```

#### Excluding Studies with Missing Observations

```{r, echo=FALSE, fig.height=10,fig.width=10}
par(mfrow = c(3,2))
for(j in 1:6){
  ranges <-  apply(cbind(outs[[1]][,j],outs[[2]][,j]), 2,
                   function(x) { dens <- density(x); c(range(dens$x), range(dens$y)) })
  plot(density(outs[[1]][,j]), col="red",
       xlim = range(ranges[1:2, ]), ylim = range(ranges[3:4, ]),
       main=paste(names(gdata)[j]))
  legend("topleft",legend = c("Mult w/o NA's ", "Uni  w/o NA's " ), lty = c(1,1), col=c("blue","red"))
  legend("topright",legend = c("2.5%","mean","97.5%" ), lty = c(2,3,2))
  lines(density(outs[[2]][,j]), col="blue")
  abline(v=mean(outs[[1]][,j]), col="red", lty=3)
  abline(v=quantile(outs[[1]][,j], .025), col="red", lty=2)
  abline(v=quantile(outs[[1]][,j], .975), col="red", lty=2)
  abline(v=mean(outs[[2]][,j]), col="blue", lty=3)
  abline(v=quantile(outs[[2]][,j], .025), col="blue", lty=2)
  abline(v=quantile(outs[[2]][,j], .975), col="blue", lty=2)  
  }
```

Contrary to what I expected, the variation in the multivariate model showed little to no improvement over the univariate.  The multivariate model produced posterior distributions with larger variance around the mean (except for Log.Nmass where the SE for the univariate model was larger by 3.3e-06, a small amount relative to the size of the mean.)


#### Including Studies with Missing Observations
```{r, echo=FALSE, fig.height=10,fig.width=10}
par(mfrow = c(3,2))
for(j in 1:6){
  ranges <-  apply(cbind(outs[[3]][,j],outs[[4]][,j]), 2,
                   function(x) { dens <- density(x); c(range(dens$x), range(dens$y)) })
  plot(density(outs[[3]][,j]), col="red",
       xlim = range(ranges[1:2, ]), ylim = range(ranges[3:4, ]),
       main=paste(names(gdata)[j]))
  legend("topleft",legend = c( "Multi w/ NA's","Uni   w/ NA's"), lty = c(1,1), col=c("blue","red"))
  legend("topright",legend = c("2.5%","mean","97.5%" ), lty = c(2,3,2))
  lines(density(outs[[4]][,j]), col="blue")
  abline(v=mean(outs[[3]][,j]), col="red", lty=3)
  abline(v=quantile(outs[[3]][,j], .025), col="red", lty=2)
  abline(v=quantile(outs[[3]][,j], .975), col="red", lty=2)
  abline(v=mean(outs[[4]][,j]), col="blue", lty=3)
  abline(v=quantile(outs[[4]][,j], .025), col="blue", lty=2)
  abline(v=quantile(outs[[4]][,j], .975), col="blue", lty=2) 
  }
```

When studies with missing observations are included, the standard errors begin to behave more like one might expect. The posterior distributions from the multivariate model have smaller variances for all the variables except Log.Pmass. However, it is difficult to see the difference since the expected means of the variables are no longer similar. 

# Continued Analysis

- Examine the posterior distributions of the estimated variances

- Account for the fact that the data is summary data

- Add random effects into the model
    - site
    - species

- Model selection score - like DIC

- Better initial conditions (?)

- Are posteriors bilogically feasible?
    - informative priors from Bety?


# References

[1] LeBauer, D.S., Wang, D., Richter, K.T., Davidson, C.C. & Dietze, M.C. Facilitating feedbacks between field measurements and ecosystem models. Ecological Monographs 83, 133-154 (2013).

[2] Wright, I.J. et al. The worldwide leaf economics spectrum. Nature 428, 821-827 (2004).

#Looking at Sigma 

Sigma
               Log.LL     Log.LMA   Log.Amass   Log.Nmass   Log.Pmass   Log.Rmass
Log.LL     0.24821316  0.10517382 -0.16248541 -0.08554957 -0.12719274 -0.12195986
Log.LMA    0.10517382  0.09018780 -0.08594849 -0.05233751 -0.07462161 -0.06535533
Log.Amass -0.16248541 -0.08594849  0.14449277  0.06872655  0.09541912  0.09386491
Log.Nmass -0.08554957 -0.05233751  0.06872655  0.05579258  0.07003691  0.05715079
Log.Pmass -0.12719274 -0.07462161  0.09541912  0.07003691  0.13286190  0.07886467
Log.Rmass -0.12195986 -0.06535533  0.09386491  0.05715079  0.07886467  0.09736912

var(gdata)
               Log.LL      Log.LMA   Log.Amass   Log.Nmass    Log.Pmass   Log.Rmass
Log.LL     0.05119710  0.030073709 -0.03351628 -0.02551575 -0.031023521 -0.03923470
Log.LMA    0.03007371  0.042225352 -0.03452770 -0.01377535 -0.009000845 -0.02487981
Log.Amass -0.03351628 -0.034527700  0.04681471  0.01949136  0.019106056  0.02973326
Log.Nmass -0.02551575 -0.013775352  0.01949136  0.03419529  0.037182592  0.03387795
Log.Pmass -0.03102352 -0.009000845  0.01910606  0.03718259  0.069031380  0.03675746
Log.Rmass -0.03923470 -0.024879812  0.02973326  0.03387795  0.036757465  0.06372909

abs(Sigma) >= var(gdata)
          Log.LL Log.LMA Log.Amass Log.Nmass Log.Pmass Log.Rmass
Log.LL      TRUE    TRUE      TRUE      TRUE      TRUE      TRUE
Log.LMA     TRUE    TRUE      TRUE      TRUE      TRUE      TRUE
Log.Amass   TRUE    TRUE      TRUE      TRUE      TRUE      TRUE
Log.Nmass   TRUE    TRUE      TRUE      TRUE      TRUE      TRUE
Log.Pmass   TRUE    TRUE      TRUE      TRUE      TRUE      TRUE
Log.Rmass   TRUE    TRUE      TRUE      TRUE      TRUE      TRUE

```{r}

```


```{r, echo=FALSE}

```

```{r, eval=FALSE}

```
