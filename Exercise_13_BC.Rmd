---
title: "Exercise_13_BC"
author: "Betsy Cowdery"
date: "December 1, 2014"
output: html_document
---

# Part 1: Ozone Concentrations

```{r, echo=FALSE}
library(spatial)          ## spatial stats library
library(maps)               ## map of US
library(xtable)
load("data/Ozone.RData")       ## data
plot(dat$Lon,dat$Lat,xlim=c(-125,-65),ylim=c(25,50),asp=70/50,pch="+",col=dat$Mean/10)
map("state",add=TRUE)     ## add the states and then a legend
legend("bottomright",legend=2:8*10,col=2:8,pch="+",cex=1.25)
surf0 <- surf.ls(0,dat$Lon,dat$Lat,dat$Mean)   ## 0-degree polynomial surface
tr0 <- trmat(surf0,-125,-65,25,50,50)          ## project a 50x50 matrix within region
image(tr0,asp=70/50)                           ## make an color image map of the surface
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
```

## Task 1

**1. Calculate the trend surface for higher order polynomials. Do higher order polynomials capture large scale trends or are all the trends finer scale? What order polynomial would you choose to detrend your data?**

```{r, echo=FALSE}
par(mfrow = c(3,2))

## 1-degree 
surf1 <- surf.ls(1,dat$Lon,dat$Lat,dat$Mean)   
tr1 <- trmat(surf1,-125,-65,25,50,50)
image(tr1,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("1-degree polynomial surface")

## 2-degree 
surf2 <- surf.ls(2,dat$Lon,dat$Lat,dat$Mean)   
tr2 <- trmat(surf2,-125,-65,25,50,50)
image(tr2,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("2-degree polynomial surface")

## 3-degree 
surf3 <- surf.ls(3,dat$Lon,dat$Lat,dat$Mean)   
tr3 <- trmat(surf3,-125,-65,25,50,50)
image(tr3,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("3-degree polynomial surface")

## 4-degree 
surf4 <- surf.ls(4,dat$Lon,dat$Lat,dat$Mean)   
tr4 <- trmat(surf4,-125,-65,25,50,50)
image(tr4,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("4-degree polynomial surface")

## 5-degree 
surf5 <- surf.ls(5,dat$Lon,dat$Lat,dat$Mean)   
tr5 <- trmat(surf5,-125,-65,25,50,50)
image(tr5,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("5-degree polynomial surface")

## 6-degree 
surf6 <- surf.ls(6,dat$Lon,dat$Lat,dat$Mean)   
tr6 <- trmat(surf6,-125,-65,25,50,50)
image(tr6,asp=70/50)                           
map("state",add=TRUE)
points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10)
title("6-degree polynomial surface")

```
From these plots, I'd choose a 3 degree polynomial - it appears to capture enough fine scale trends. 


```{r,echo=FALSE}
vg <- variogram(surf3,300,xlim=c(0,10),ylim=c(0,1.1*var(dat$Mean)))
title("Variogram")
abline(h=var(dat$Mean))                   ## asymptotic variance
cg<- correlogram(surf3,300,xlim=c(0,10))  ## limit the range of radii to the initial decay
title("Correlogram")
```

## Task 2

**2. EXTRA CREDIT: There are two other correlation functions built into the spatial package, the Gaussian covariance gaucov and the spherical covariance sphercov. Derive MLE for these two functions via numerical optimization. Turn in an AIC table for the models you fit and identify the best fitting model that you will use for Kriging**

```{r, echo=FALSE}
par(mfrow = c(1,1))
rng <- 1:10                   ## define the distance bins we want to use for the fit
theta <- c(2,0.1,1,0.01)
expfit <- function(theta){    ## negative log likelihood function
  d = theta[1]
  alpha = theta[2]
  se = theta[3]
  sigma = theta[4]
  -sum(dnorm(cg$y[rng],expcov(cg$x[rng],d,alpha,se),sigma,log=TRUE)) 
}
efit <- optim(c(2,0.1,1,0.01),expfit)    ## numerical optimization

gfit <- function(theta){    ## negative log likelihood function
  d = theta[1]
  alpha = theta[2]
  se = theta[3]
  sigma = theta[4]
  -sum(dnorm(cg$y[rng],gaucov(cg$x[rng],d,alpha,se),sigma,log=TRUE)) 
}
gfit <- optim(par = theta,fn = gfit)    ## numerical optimization

D <- 2
sfit <- function(theta){    ## negative log likelihood function
  d = theta[1]
  alpha = theta[2]
  se = theta[3]
  sigma = theta[4]
  -sum(dnorm(cg$y[rng],sphercov(cg$x[rng],d,alpha,se,D),sigma,log=TRUE)) 
}
sfit <- optim(par = theta,fn = sfit)    ## numerical optimization

par(lwd=3,cex=1.1)
cg <- correlogram(surf4,300,xlim=c(0,10))
lines(cg$x,expcov(cg$x,efit$par[1],efit$par[2],efit$par[3]),lwd=3)
lines(cg$x,gaucov(cg$x,gfit$par[1],gfit$par[2],gfit$par[3]),col=2,lwd=3)
lines(cg$x,sphercov(cg$x,sfit$par[1],sfit$par[2],sfit$par[3],sfit$par[4]),col=3,lwd=3)
legend("topright",legend=c("Exponential","Gaussian","Spherical"),col=1:3,lwd=2)
```

**3. Explain the figure generated from the previous box of code. What is being plotted (axes, data, model)? How would you 2interpret this figure? How would you interpret the parameters of the best-fit model? What is the range? Note: you do not need to include the Gaussian and spherical lines if you did not fit them in the previous step.**

This is a plot of the autocorrelation of the data, which in the context of spatial data means whether or not the data is spatially independent. The positively correlated points suggests clustering (which makes sense if we look at the plot on the map and see what looks like clusterd data around the east and west coasts etc. )

All the curves are very close to each other so I can't really see much of a benefit of using one of the other. In this case I'll use the Gaussian becuase it seems like a good average between the other two. 

## Task 3
**4. Include your kriged map of mean US ozone for 2008 in your lab report. Where in the country has the highest ozone levels?**

# ```{r, echo=FALSE}
# KrExp <- surf.gls(3,expcov,dat$Lon,dat$Lat,dat$Mean,
#            d=efit$par[1],alpha=efit$par[2],se=efit$par[3])  ##construct surface
# kexp3 <- prmat(KrExp,-125,-65,25,50,100)                      ## predict to a matrix
# image(kexp3,main="Krige Prediction Exponential",cex.main=2,asp=70/50)                ## image of predictions
# contour(kexp3,add=TRUE,lwd=1) ## conv to ppb                  ## contours of prediction
# map("state",add=TRUE)
# points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10,cex=0.5)
# ```

```{r, echo=FALSE}
KrGau <- surf.gls(3,gaucov,dat$Lon,dat$Lat,dat$Mean,
           d=gfit$par[1],alpha=gfit$par[2],se=gfit$par[3])  ##construct surface
pr3 <- prmat(KrGau,-125,-65,25,50,100)                      ## predict to a matrix
image(pr3,main="Krige Prediction Gaussian",cex.main=2,asp=70/50)                ## image of predictions
contour(pr3,add=TRUE,lwd=1) ## conv to ppb                  ## contours of prediction
map("state",add=TRUE)
#points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10,cex=0.5)
```

It looks as though the biggest source of ozone is the atlantic ocean, however I think this is an artifact of the Kriging process.  

If we restrict our observatiosn to just the US then the largest sources of ozone appear to be in the western united states. Especially from the four corner states and lower california. There are also some high concentrations in the east in Tennessee and North Carolina. 

# ```{r, echo=FALSE}
# KrSur <- surf.gls(3,sphercov,dat$Lon,dat$Lat,dat$Mean,
#            d=sfit$par[1],alpha=sfit$par[2],se=sfit$par[3])  ##construct surface
# pr3 <- prmat(KrSur,-125,-65,25,50,100)                      ## predict to a matrix
# image(pr3,main="Krige Prediction",cex.main=2,asp=70/50)                ## image of predictions
# contour(pr3,add=TRUE,lwd=1) ## conv to ppb                  ## contours of prediction
# map("state",add=TRUE)
# points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10,cex=0.5)
# ```


**Generate a second map of mean US ozone but this time include contours based on the error matrix rather than the prediction matrix. Where are ozone levels most uncertain?**

```{r, echo=FALSE}
se3 <- semat(KrGau,-125,-65,25,50,150)                      ## error matrix
image(se3,main="Krige Standard Error",cex.main=2,asp=70/50)                ## image of errors
contour(se3,add=TRUE,lwd=1) ## conv to ppb  
map("state",add=TRUE)

# se3 <- semat(KrExp,-125,-65,25,50,150)                      ## error matrix
# image(se3,main="Krige",cex.main=2,asp=70/50)                ## image of errors
# contour(se3,add=TRUE,lwd=1) ## conv to ppb  
# map("state",add=TRUE)
# points(dat$Lon,dat$Lat,pch="+",col=dat$Mean/10,cex=0.5)
```

The highest error follows where tehre are no data points - like for example in the ocean where we were getting unrealistic predictions.  

**.5 There are a few reasons that one might be skeptical of this initial exploratory map. Name at least two and describe ways the model could be improved (without collecting additional data).**

- We did not explore the possibility of anistropy. It is possible that the spacial covariance is not the same in all directions. 

- The variogram does not fit the standard curve we had looked at before - it doesn't converge nicely to the sill and has a large nugget value. 

- My initial choice for the order of the polynomial of the trend was never proven to be the "correct" ie optimal value. 

# Part 2:National Ozone Time Series

## Task 4

**7. Include the time series plot and briefly describe the features of the overall trend in ozone concentrations.**

```{r, echo=FALSE}
xt <- ts(ozone$Mean,start=1980,end=2008)  ## convert data to a time series object

k <- c(0.1,0.2,0.4,0.2,0.1)      ## kernel
fx <- filter(xt,k)               ## weighted moving average
lx <- lowess(xt,f=1/3)
plot(xt,type='b',ylab="ppm",xlab="Year",main="National Ozone Concentrations")
lines(fx,col=3)
lines(lx,col=2)
legend("topright",legend=c("Weighted Moving Average", "Lowess Curve"), lty=c(1,1), col=c(3,2))
```

The data is trending downwards in what could be fitted with a line, however, the moving average and lowess curve seem to show an ocilation that might be better fit with a higher order polynomial. 

## Task 5

**8. Does the detrended data meet the assumptions of second order stationarity? Why or why not?**

```{r,echo=FALSE}
rx = xt - lx$y        ## residuals around lowess curve
plot(rx)              ## check for homoskedasticity
abline(h=0,lty=2,col=2)
hist(rx,10)              ## check for a normal distribution with mean=zero
abline(v=0,lty=2,col=2)

## Quantile-Quantile plot (by hand)
n = length(rx)
qseq = seq(0.5/n,length=n,by=1/n)
plot(qnorm(qseq,0,sd(rx)),sort(rx),main="Quantile-Quantile")
abline(0,1,lty=2)
```

Overall, I would be warry of saying this is stationary, while the mean is close to 0, the histogram does appeared to be skewed to the right - this can also be seen on both sides,  expecially the right hand side of the Q-Q plot. Thus the homeskedastic variance will be non-zero. It would be worth doing a reanalysis in which we identify potential outliers. 


**9. Does the first-difference time series meet the assumptions of second order stationarity? Why or why not?**

```{r,echo=FALSE}
dxt = diff(xt)
plot(dxt)
abline(h=0,lty=2,col=2)
hist(dxt,10)
abline(v=0,lty=2,col=2)

n = length(dxt)
qseq = seq(0.5/n,length=n,by=1/n)
plot(qnorm(qseq,0,sd(dxt)),sort(dxt),main="Quantile-Quantile")
abline(0,1,lty=2)
```

Once again, I'm skeptical because even thought the mean residual error near 0, there is a bias present that that results in a non-zero homeskedastic variance. Also, even without the bias, the histogram doens't make a very clean normal curve. 

Autocorrelation and partial autocorrelation in the time series:
```{r,echo=FALSE}
par(mfrow = c(2,3))
acf(xt, main ="ACF on the original time series" )    ## ACF on the original time series
acf(rx, main = "ACF on the detrended data")    ## ACF on the detrended data
acf(dxt, main="ACF on the first difference series")   ## ACF on the first difference series

pacf(xt, main = "Partial ACF of the time series")   ## Partial ACF of the time series
pacf(rx, main = "Partial ACF of the detrended data")   ## Partial ACF of the detrended data
pacf(dxt, main = "Partial ACF of the first differences")  ## Partial ACF of the first differences
```



## Task 7 

**12. Based on the diagnostics performed, what ARIMA model is likely to perform best? What orders should p, d, and q be? Why? Should you fit the model to xt or rx? **


Assuming that differencing once does in fact satisfy stationarity, set d= 1

To account for autocorrelated error in the stationarized series, set q = 1. 



**13. Fit the arima model you proposed using the function arima:**

```{r, eval=FALSE}
# plot(rx)
# abline(h=0,lty=2)
# hist(rx,10)
# 
# plot(diff(rx))
# abline(h=0,lty=2)
# hist(diff(rx,10))
# 
# acf(diff(rx))
# pacf(diff(rx))
# 

p <- 0
d <- 1
q <- 0
arima(xt,c(0,1,1)) # my guess - aic = -225.47


```
**Then propose alternative models that are similar to the one you fit (e.g. increase or decrease orders by 1). Based on AIC scores what model provides the best fit? Provide a table of models you tried and their AIC scores.**

It appears as though differencing once is enough - the test with d=2 had a lower aic. Also my arima choice did better than a random walk.

```{r, echo=FALSE}

comp <- as.data.frame(rbind(
    c(sprintf("p= %i,d=%i,q=%i | ",0,1,1), arima(xt,c(0,1,1))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",0,2,1), arima(xt,c(0,2,1))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",1,1,0), arima(xt,c(1,1,0))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",0,1,0), arima(xt,c(0,1,0))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",0,0,0), arima(xt,c(0,0,0))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",1,1,1), arima(xt,c(1,1,1))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",2,1,1), arima(xt,c(2,1,1))$aic),
    c(sprintf("p= %i,d=%i,q=%i | ",0,0,1), arima(xt,c(0,0,1))$aic)
    ))

tab <- xtable(comp)
print(comp)

```
